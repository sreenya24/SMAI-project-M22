{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOW():\n",
    "    def __init__(self, k_val, b_val):\n",
    "        # Constants\n",
    "        self.k_val = k_val\n",
    "        self.b_val = b_val\n",
    "\n",
    "        # Vocabulary contains the words in the corpus and the number of documents they appear in\n",
    "        self.vocabulary = {}\n",
    "        self.vocabulary_arr = None\n",
    "        self.vocab_size = None\n",
    "        \n",
    "        # Documents contains the words in each document and the number of times they appear\n",
    "        self.documents = {}\n",
    "        self.document_vectors = None\n",
    "        self.doc_count = None\n",
    "        self.max_docs = None\n",
    "        self.avg_doc_length = None\n",
    "\n",
    "        # score matrices\n",
    "        self.tf_matrix = None\n",
    "        self.idf_matrix = None\n",
    "    \n",
    "    def create(self, path_to_annotations):\n",
    "\n",
    "        self.doc_count = 0\n",
    "        for (root, dirs, files) in os.walk(path_to_annotations, topdown=True):\n",
    "            print(root)\n",
    "            # print(dirs)\n",
    "            # print(files)\n",
    "            for file in files:\n",
    "                file_num = file.split('.')[0]\n",
    "                file_num = int(file_num)\n",
    "                # parses the annotation file and adds the words to the vocabulary\n",
    "                self.parse_annotation_file(os.path.join(root, file), file_num)\n",
    "                self.doc_count += 1\n",
    "\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "\n",
    "        self.vocabulary_arr = np.array(list(self.vocabulary))\n",
    "\n",
    "        # make document vectors\n",
    "        self.max_docs = max(self.documents.keys())+1\n",
    "        self.document_vectors = np.zeros((self.max_docs, self.vocab_size))\n",
    "        \n",
    "        for doc_num in self.documents.keys():\n",
    "            for word in self.documents[doc_num]:\n",
    "                word_index = np.where(self.vocabulary_arr == word)[0][0]\n",
    "                self.document_vectors[doc_num][word_index] = self.documents[doc_num][word]\n",
    "\n",
    "        self.avg_doc_length = np.sum(self.document_vectors) / self.doc_count\n",
    "\n",
    "    def form_matrix(self):\n",
    "        if(self.vocabulary_arr is None):\n",
    "            print(\"vocabulary is none\")\n",
    "            print(\"run create() first\")\n",
    "\n",
    "        # calculate idf\n",
    "        values = self.vocabulary.values()\n",
    "        values = np.array(list(values))\n",
    "        self.idf_matrix = np.log((self.doc_count + 1) / (values + 0.5))\n",
    "\n",
    "        # calculate tf\n",
    "        self.tf_matrix = self.k_val*self.document_vectors / (self.k_val * (1 - self.b_val + (self.b_val * (np.sum(self.document_vectors,axis=1).T/ self.avg_doc_length))) + self.document_vectors.T).T   \n",
    "\n",
    "        return self.tf_matrix, self.idf_matrix  \n",
    "\n",
    "    def get_query_score(self, path_to_query, doc_num):\n",
    "        if(self.idf_matrix is None):\n",
    "            print(\"idf matrix is none\")\n",
    "            print(\"run form_matrix() first\")\n",
    "        if(self.tf_matrix is None):\n",
    "            print(\"tf matrix is none\")\n",
    "            print(\"run form_matrix() first\")\n",
    "\n",
    "        vector = self.get_query_vector(path_to_query)\n",
    "        tf_q = self.get_query_tf(vector)\n",
    "        tf_d = self.tf_matrix[doc_num]\n",
    "        idf = self.idf_matrix\n",
    "        return np.sum(tf_q*tf_d*idf*idf)\n",
    "\n",
    "    def get_query_tf(self, query_vector):\n",
    "        return self.k_val*query_vector / (self.k_val * (1 - self.b_val + self.b_val * (np.sum(query_vector).T/ self.avg_doc_length)) + query_vector.T).T\n",
    "\n",
    "    def get_query_vector(self, path_to_query):\n",
    "        query_tf_vector = np.zeros(self.vocab_size)\n",
    "        with open(path_to_query, 'r', encoding='latin-1') as f:\n",
    "            # read the doc as a string\n",
    "            doc = f.read()\n",
    "            \n",
    "            # get the words in the doc\n",
    "            words_in_doc = self.get_words_in_doc(doc)\n",
    "\n",
    "            for word in words_in_doc:\n",
    "                word_index = np.where(self.vocabulary_arr == word)[0][0]\n",
    "                query_tf_vector[word_index] = words_in_doc[word]\n",
    "            \n",
    "        return query_tf_vector\n",
    "\n",
    "    def parse_annotation_file(self, path_to_annotation_file, file_number):\n",
    "        # open in latin-1 encoding to avoid UnicodeDecodeError\n",
    "        with open(path_to_annotation_file, 'r', encoding='latin-1') as f:\n",
    "            # read the doc as a string\n",
    "            doc = f.read()\n",
    "            \n",
    "            # get the words in the doc\n",
    "            words_in_doc = self.get_words_in_doc(doc)\n",
    "            \n",
    "            # add the words in the document to the vocabulary\n",
    "            self.documents[file_number] = words_in_doc\n",
    "\n",
    "            for word in words_in_doc:\n",
    "                if word in self.vocabulary:\n",
    "                    self.vocabulary[word] += 1\n",
    "                else:\n",
    "                    self.vocabulary[word] = 1\n",
    "\n",
    "    def get_words_in_doc(self, doc):\n",
    "\n",
    "        # replace \\n with space\n",
    "        doc = doc.replace('\\n', ' ')\n",
    "        # get parts between all <TITLE> and </TITLE> tags\n",
    "        titles = re.findall(r'<TITLE>(.*?)</TITLE>', doc)\n",
    "        # get parts between all <DESCRIPTION> and </DESCRIPTION> tags\n",
    "        descriptions = re.findall(r'<DESCRIPTION>(.*?)</DESCRIPTION>', doc)\n",
    "        # get parts between all <NOTES> and </NOTES> tags\n",
    "        notes = re.findall(r'<NOTES>(.*?)</NOTES>', doc)\n",
    "        # get parts between all <LOCATION> and </LOCATION> tags\n",
    "        locations = re.findall(r'<LOCATION>(.*?)</LOCATION>', doc)\n",
    "\n",
    "        words_in_doc = {}\n",
    "\n",
    "        # split into words and add to vocabulary\n",
    "        for title in titles:\n",
    "            for word in title.split():\n",
    "                word = word.strip(' .,;:!?()[]\\{\\}\\'\\\"')\n",
    "                word = word.lower()\n",
    "                if word not in words_in_doc:\n",
    "                    words_in_doc[word] = 0\n",
    "                words_in_doc[word] += 1\n",
    "\n",
    "        for description in descriptions:\n",
    "            for word in description.split():\n",
    "                word = word.strip('.,;:!?()[]\\{\\}\\'\\\"')\n",
    "                word = word.lower()\n",
    "                if word not in words_in_doc:\n",
    "                    words_in_doc[word] = 0\n",
    "                words_in_doc[word] += 1\n",
    "\n",
    "        for note in notes:\n",
    "            for word in note.split():\n",
    "                word = word.strip('.,;:!?()[]\\{\\}\\'\\\"')\n",
    "                word = word.lower()\n",
    "                if word not in words_in_doc:\n",
    "                    words_in_doc[word] = 0\n",
    "                words_in_doc[word] += 1\n",
    "\n",
    "        for location in locations:\n",
    "            for word in location.split():\n",
    "                word = word.strip('.,;:!?()[]\\{\\}\\'\\\"')\n",
    "                word = word.lower()\n",
    "                if word not in words_in_doc:\n",
    "                    words_in_doc[word] = 0\n",
    "                words_in_doc[word] += 1\n",
    "\n",
    "        return words_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Smol_set/annotations_complete_eng/\n",
      "./Smol_set/annotations_complete_eng/00\n",
      "./Smol_set/annotations_complete_eng/01\n",
      "./Smol_set/annotations_complete_eng/02\n",
      "./Smol_set/annotations_complete_eng/03\n",
      "./Smol_set/annotations_complete_eng/04\n",
      "./Smol_set/annotations_complete_eng/05\n"
     ]
    }
   ],
   "source": [
    "path_to_annotations = './Smol_set/annotations_complete_eng/'\n",
    "k_val = 1.5\n",
    "b_val = 0.75\n",
    "bow = BOW(k_val, b_val)\n",
    "bow.create(path_to_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix, idf_matrix = bow.form_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.53875972166996\n"
     ]
    }
   ],
   "source": [
    "sample = bow.get_query_score('./Smol_set/annotations_complete_eng/03/3489.eng',3489)\n",
    "\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef920dd73cd3ef78477e8ab32607aaeccce0de781c75bf88db90c589e446cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
